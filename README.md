# RAG Melhorado com LangChain e LangGraph

Este projeto implementa um **Retrieval-Augmented Generation (RAG) melhorado**, utilizando LangChain e LangGraph para criar um fluxo estruturado que busca fornecer respostas mais precisas e relevantes.

## üìå Objetivo

A proposta deste projeto √© criar um **RAG mais eficiente**, onde o fluxo de obten√ß√£o de informa√ß√µes √© aprimorado por meio de **LangGraph**. A abordagem permite estruturar a l√≥gica de recupera√ß√£o e gera√ß√£o de respostas em **fluxogramas**, garantindo maior controle e refinamento na busca de informa√ß√µes.

## üõ† Tecnologias Utilizadas

- **Python** 
- **LangChain**
- **LangGraph**
- **Ollama** (ou outro LLM compat√≠vel)
- **FAISS**

## üìÇ Estrutura do Projeto

O notebook segue um fluxo estruturado para construir o RAG aprimorado:

1. **Configura√ß√£o Inicial**: Importa√ß√£o das bibliotecas necess√°rias.
2. **Defini√ß√£o do Fluxo no LangGraph**: Implementa√ß√£o da estrutura do pipeline de recupera√ß√£o e gera√ß√£o de respostas.
3. **Configura√ß√£o do Modelo**: Integra√ß√£o com LLM Ollama.
4. **Integra√ß√£o com FAISS**: Para indexa√ß√£o e busca de documentos.
5. **Execu√ß√£o do Pipeline**: Testes e avalia√ß√£o das respostas geradas.

## üöÄ Como Utilizar

### 1Ô∏è‚É£ Instale as Depend√™ncias

Antes de executar o notebook, instale as bibliotecas necess√°rias que se encontram no pr√≥prio notebook.

### 2Ô∏è‚É£ Instale e Configure o Ollama
Este projeto utiliza **Ollama** para rodar modelos localmente. Siga os passos abaixo:

1. Baixe e instale o Ollama seguindo as instru√ß√µes do site oficial: [ollama.com](https://ollama.com)
2. Baixe os modelos necess√°rios executando os comandos abaixo:
    
    ```bash
    ollama pull llama3
    ollama pull llama3:3.1
    ```
3. Certifique-se de que o servidor do Ollama est√° rodando antes de executar o notebook.
    

### 3Ô∏è‚É£ Execute o Notebook

Basta rodar as c√©lulas do notebook para testar o fluxo.

## üß† Modelos Utilizados

Este projeto utiliza **modelos do Ollama**, comparando os resultados com o uso de uma √∫nica LLM do Ollama.
O objetivo √© verificar se essa abordagem gera respostas mais precisas e coerentes.

## üìå Considera√ß√µes Finais

Este projeto demonstra como um **RAG pode ser melhorado utilizando LangGraph**, criando um fluxo estruturado e refinando a gera√ß√£o de respostas. Voc√™ pode personalizar a abordagem integrando novos **LLMs, bases de dados vetoriais e regras de fluxo**.
